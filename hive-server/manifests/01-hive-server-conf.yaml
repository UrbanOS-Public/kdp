---
apiVersion: v1
kind: ConfigMap
metadata:
  name: hive-site
  labels:
    app: hive-server
data:
  hive-site.xml: |-
    <?xml version="1.0" encoding="UTF-8" standalone="no"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>hive.metastore.uris</name>
        <value>thrift://hive-metastore:9083</value>
      </property>
      <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>s3a://jeff-jarred-751/hive-s3</value>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:postgresql://postgres/metastore</value>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>org.postgresql.Driver</value>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>hive</value>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>password123</value>
      </property>
      <property>
        <name>datanucleus.autoCreateSchema</name>
        <value>true</value>
      </property>
      <property>
        <name>datanucleus.fixedDatastore</name>
        <value>true</value>
      </property>
      <property>
        <name>datanucleus.autoCreateTables</name>
        <value>True</value>
      </property>
      <property>
        <name>hive.execution.engine</name>
        <value>spark</value>
      </property>
      <property>
        <name>spark.master</name>
        <value>k8s://https://kubernetes.default.svc.cluster.local:443</value>
      </property>
      <property>
        <name>spark.eventLog.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>spark.eventLog.dir</name>
        <value>/tmp</value>
      </property>
      <property>
        <name>spark.executor.memory</name>
        <value>512m</value>
      </property>
      <property>
        <name>spark.serializer</name>
        <value>org.apache.spark.serializer.KryoSerializer</value>
      </property>
      <property>
        <name>spark.kubernetes.namespace</name>
        <value>default</value>
      </property>
      <property>
        <name>spark.kubernetes.container.image</name>
        <value>jeffgrunewald/de-doop:1.17</value>
      </property>
      <property>
        <name>spark.kubernetes.container.image.pullPolicy</name>
        <value>IfNotPresent</value>
      </property>
      <property>
        <name>spark.executor.instances</name>
        <value>2</value>
      </property>
      <property>
        <name>spark.kubernetes.allocation.batch.size</name>
        <value>2</value>
      </property>
      <property>
        <name>spark.kubernetes.allocation.batch.delay</name>
        <value>10s</value>
      </property>
      <property>
        <name>hive.spark.client.rpc.server.address</name>
        <value>POD_IP.default.pod.cluster.local</value>
      </property>
      <property>
        <name>spark.kubernetes.driver.label.seatOf</name>
        <value>pants</value>
      </property>
      <property>
        <name>spark.kubernetes.executor.label.seatOf</name>
        <value>pants</value>
      </property>
      <property>
        <name>spark.app.name</name>
        <value>de-doop</value>
      </property>
      <property>
        <name>spark.submit.deployMode</name>
        <value>cluster</value>
      </property>
      <property>
        <name>spark.driver.extraLibraryPath</name>
        <value>/opt/hadoop/lib/native</value>
      </property>
      <property>
        <name>spark.executor.extraJavaOptions</name>
        <value>-Dlog4j.configuration=file:///opt/spark/conf/log4j.properties</value>
      </property>
      <property>
        <name>spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-pvc.mount.path</name>
        <value>/tmp/hive/root/</value>
      </property>
      <property>
        <name>spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-pvc.options.claimName</name>
        <value>spark-pvc</value>
      </property>
      <property>
        <name>spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-pvc.mount.readOnly</name>
        <value>false</value>
      </property>
      <property>
        <name>spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-pvc.mount.path</name>
        <value>/tmp/hive/root/</value>
      </property>
      <property>
        <name>spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-pvc.mount.readOnly</name>
        <value>false</value>
      </property>
      <property>
        <name>spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-pvc.options.claimName</name>
        <value>spark-pvc</value>
      </property>
    </configuration>
